{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0923615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8da6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bastin/Projekt\n"
     ]
    }
   ],
   "source": [
    "cd /home/bastin/Projekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfbf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moco.builder import MoCo\n",
    "import math\n",
    "import sys\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f094a8",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2a1838",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-345a3e08aed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/bastin/PROJECT-main/Data/DF2K_Train/HR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = datasets.ImageFolder('/home/bastin/PROJECT-main/Data/DF2K_Train/HR', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ca0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c3ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c92864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c270d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e94be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99f489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051562a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed5051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865748d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf51e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf299fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339204da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ce8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31712fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16272491",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common functions that will be used i the model : default_convolution, Meanshift, Upsmapler\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
    "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
    "        self.bias.data.div_(std)\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias.requires_grad = False\n",
    "\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, conv, scale, n_feat, act=False, bias=True):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:    # Is scale = 2^n?\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(conv(n_feat, 4 * n_feat, 3, bias))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "                if act: m.append(act())\n",
    "        elif scale == 3:\n",
    "            m.append(conv(n_feat, 9 * n_feat, 3, bias))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "            if act: m.append(act())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        super(Upsampler, self).__init__(*m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DA_conv(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, kernel_size, reduction):\n",
    "        super(DA_conv, self).__init__()\n",
    "        self.channels_out = channels_out\n",
    "        self.channels_in = channels_in\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.kernel = nn.Sequential(\n",
    "            nn.Linear(64, 64, bias=False),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Linear(64, 64 * self.kernel_size * self.kernel_size, bias=False)\n",
    "        )\n",
    "        self.conv = default_conv(channels_in, channels_out, 1)\n",
    "        self.ca = CA_layer(channels_in, channels_out, reduction)\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.1, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x[0]: feature map: B * C * H * W\n",
    "        :param x[1]: degradation representation: B * C\n",
    "        '''\n",
    "        b, c, h, w = x[0].size()\n",
    "\n",
    "        # branch 1\n",
    "        kernel = self.kernel(x[1]).view(-1, 1, self.kernel_size, self.kernel_size)\n",
    "        out = self.relu(F.conv2d(x[0].view(1, -1, h, w), kernel, groups=b*c, padding=(self.kernel_size-1)//2))\n",
    "        out = self.conv(out.view(b, -1, h, w))\n",
    "\n",
    "        # branch 2\n",
    "        out = out + self.ca(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CA_layer(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, reduction):\n",
    "        super(CA_layer, self).__init__()\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, channels_in//reduction, 1, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Conv2d(channels_in // reduction, channels_out, 1, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x[0]: feature map: B * C * H * W\n",
    "        :param x[1]: degradation representation: B * C\n",
    "        '''\n",
    "        att = self.conv_du(x[1][:, :, None, None])\n",
    "\n",
    "        return x[0] * att\n",
    "\n",
    "\n",
    "class DAB(nn.Module):\n",
    "    def __init__(self, conv, n_feat, kernel_size, reduction):\n",
    "        super(DAB, self).__init__()\n",
    "\n",
    "        self.da_conv1 = DA_conv(n_feat, n_feat, kernel_size, reduction)\n",
    "        self.da_conv2 = DA_conv(n_feat, n_feat, kernel_size, reduction)\n",
    "        self.conv1 = conv(n_feat, n_feat, kernel_size)\n",
    "        self.conv2 = conv(n_feat, n_feat, kernel_size)\n",
    "\n",
    "        self.relu =  nn.LeakyReLU(0.1, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x[0]: feature map: B * C * H * W\n",
    "        :param x[1]: degradation representation: B * C\n",
    "        '''\n",
    "\n",
    "        out = self.relu(self.da_conv1(x))\n",
    "        out = self.relu(self.conv1(out))\n",
    "        out = self.relu(self.da_conv2([out, x[1]]))\n",
    "        out = self.conv2(out) + x[0]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DAG(nn.Module):\n",
    "    def __init__(self, conv, n_feat, kernel_size, reduction, n_blocks):\n",
    "        super(DAG, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        modules_body = [\n",
    "            DAB(conv, n_feat, kernel_size, reduction) \\\n",
    "            for _ in range(n_blocks)\n",
    "        ]\n",
    "        modules_body.append(conv(n_feat, n_feat, kernel_size))\n",
    "\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x[0]: feature map: B * C * H * W\n",
    "        :param x[1]: degradation representation: B * C\n",
    "        '''\n",
    "        res = x[0]\n",
    "        for i in range(self.n_blocks):\n",
    "            res = self.body[i]([res, x[1]])\n",
    "        res = self.body[-1](res)\n",
    "        res = res + x[0]\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class DASR(nn.Module):\n",
    "    def __init__(self,conv=default_conv):\n",
    "        super(DASR, self).__init__()\n",
    "\n",
    "        self.n_groups = 5\n",
    "        n_blocks = 5\n",
    "        n_feats = 64\n",
    "        kernel_size = 3\n",
    "        reduction = 8\n",
    "        scale = int(4.0)\n",
    "\n",
    "        # RGB mean for DIV2K\n",
    "        rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "        rgb_std = (1.0, 1.0, 1.0)\n",
    "        self.sub_mean = MeanShift(255.0, rgb_mean, rgb_std)\n",
    "        self.add_mean = MeanShift(255.0, rgb_mean, rgb_std, 1)\n",
    "\n",
    "        # head module\n",
    "        modules_head = [conv(3, n_feats, kernel_size)]\n",
    "        self.head = nn.Sequential(*modules_head)\n",
    "\n",
    "        # compress\n",
    "        self.compress = nn.Sequential(\n",
    "            nn.Linear(256, 64, bias=False),\n",
    "            nn.LeakyReLU(0.1, True)\n",
    "        )\n",
    "\n",
    "        # body\n",
    "        modules_body = [\n",
    "            DAG(default_conv, n_feats, kernel_size, reduction, n_blocks) \\\n",
    "            for _ in range(self.n_groups)\n",
    "        ]\n",
    "        modules_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "        # tail\n",
    "        modules_tail = [Upsampler(conv, scale, n_feats, act=False),\n",
    "                        conv(n_feats, 3, kernel_size)]\n",
    "        self.tail = nn.Sequential(*modules_tail)\n",
    "\n",
    "    def forward(self, x, k_v):\n",
    "        k_v = self.compress(k_v)\n",
    "\n",
    "        # sub mean\n",
    "        x = self.sub_mean(x)\n",
    "\n",
    "        # head\n",
    "        x = self.head(x)\n",
    "\n",
    "        # body\n",
    "        res = x\n",
    "        for i in range(self.n_groups):\n",
    "            res = self.body[i]([res, k_v])\n",
    "        res = self.body[-1](res)\n",
    "        res = res + x\n",
    "\n",
    "        # tail\n",
    "        x = self.tail(res)\n",
    "\n",
    "        # add mean\n",
    "        x = self.add_mean(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.E = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.1, True),\n",
    "            nn.Linear(256, 256),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea = self.E(x).squeeze(-1).squeeze(-1)\n",
    "        out = self.mlp(fea)\n",
    "\n",
    "        return fea, out\n",
    "\n",
    "\n",
    "class BlindSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlindSR, self).__init__()\n",
    "\n",
    "        # Generator\n",
    "        self.G = DASR()\n",
    "\n",
    "        # Encoder\n",
    "        self.E = MoCo(base_encoder=Encoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            x_query = x[:, 0, ...]                          # b, c, h, w\n",
    "            x_key = x[:, 1, ...]                            # b, c, h, w\n",
    "\n",
    "            # degradation-aware represenetion learning\n",
    "            fea, logits, labels = self.E(x_query, x_key)\n",
    "\n",
    "            # degradation-aware SR\n",
    "            sr = self.G(x_query, fea)\n",
    "\n",
    "            return sr, logits, labels\n",
    "        else:\n",
    "            # degradation-aware represenetion learning\n",
    "            fea = self.E(x, x)\n",
    "\n",
    "            # degradation-aware SR\n",
    "            sr = self.G(x, fea)\n",
    "\n",
    "            return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e162e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlindSR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b393334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simclr.simclr import SimCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62048238",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'encoder', 'projection_dim', and 'n_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-95d6b5f8befd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimCLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'encoder', 'projection_dim', and 'n_features'"
     ]
    }
   ],
   "source": [
    "E = SimCLR(encoder = Encoder,projection_dim = 64, n_features = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c395e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd04e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58896e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c60bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf09b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
